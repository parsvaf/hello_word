{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "breakout_DQN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parsvaf/hello_word/blob/master/breakout_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJB7OBx4LnlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHKVZ9QWnt5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiqbw8lkrVR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujHXif-RpiTF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f2d4504-1cb0-4ced-ab8d-899074f32a36"
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbedUvLS7X3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw6348u8n6CE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "45bdbfc4-01a0-4f88-b55c-ff599d6c1ac6"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0706 19:05:03.746556 139871161165696 abstractdisplay.py:151] xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1051'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1051'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CIgDbvZ-zel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"original code:\n",
        "github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import gym\n",
        "from gym import spaces\n",
        "import cv2\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        if isinstance(env.action_space, gym.spaces.MultiBinary):\n",
        "            self.noop_action = np.zeros(self.env.action_space.n, dtype=np.int64)\n",
        "        else:\n",
        "            # used for atari environments\n",
        "            self.noop_action = 0\n",
        "            assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def _reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def _reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done  = True\n",
        "\n",
        "    def _step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def _reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype='uint8')\n",
        "        self._skip       = skip\n",
        "\n",
        "    def _step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def _reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = 84\n",
        "        self.height = 84\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.height, self.width, 1))\n",
        "\n",
        "    def _observation(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        return frame[:, :, None]\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        See Also\n",
        "        --------\n",
        "        baselines.common.atari_wrappers.LazyFrames\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k))\n",
        "\n",
        "    def _reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def _step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def _observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\n",
        "        This object should only be converted to numpy array before being passed to the model.\n",
        "        You'd not belive how complex the previous solution was.\"\"\"\n",
        "        self._frames = frames\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = np.concatenate(self._frames, axis=2)\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "def make_atari(env_id):\n",
        "    env = gym.make(env_id)\n",
        "    assert 'NoFrameskip' in env.spec.id\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    return env\n",
        "\n",
        "def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\n",
        "    \"\"\"Configure environment for DeepMind-style Atari.\n",
        "    \"\"\"\n",
        "    if episode_life:\n",
        "        env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = WarpFrame(env)\n",
        "    if scale:\n",
        "        env = ScaledFloatFrame(env)\n",
        "    if clip_rewards:\n",
        "        env = ClipRewardEnv(env)\n",
        "    if frame_stack:\n",
        "        env = FrameStack(env, 4)\n",
        "    return env\n",
        "\n",
        "def make_wrap_atari(env_id='Breakout-v0', clip_rewards=True):\n",
        "    #env = gym.make(env_id)\n",
        "    env = make_atari(env_id)\n",
        "    return wrap_deepmind(env, clip_rewards=clip_rewards, frame_stack=True, scale=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPh5GFVq5TQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI2033O0Ly_5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64ce6c20-6b1d-4018-818e-f9b04c593418"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "!ls\n",
        "\n",
        "\n",
        "import argparse\n",
        "import test\n",
        "\n",
        "import numpy as np\n",
        "# from atari_wrapper import make_wrap_atari\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Environment(object):\n",
        "    def __init__(self, atari_wrapper=False, test=False):\n",
        "        if atari_wrapper:\n",
        "            clip_rewards = not test\n",
        "            self.env = make_wrap_atari(\"BreakoutNoFrameskip-v4\", clip_rewards)\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "        self.action_space = self.env.action_space\n",
        "        self.observation_space = self.env.observation_space\n",
        "        print(self.observation_space)\n",
        "        self.env = self.wrap_env(self.env)\n",
        "\n",
        "        \n",
        "     \n",
        "      \n",
        "    def wrap_env(self,env):\n",
        "         env = Monitor(env, './video', force=True)\n",
        "         return env\n",
        "    def seed(self, seed):\n",
        "        '''\n",
        "        Control the randomness of the environment\n",
        "        '''\n",
        "        self.env.seed(seed)\n",
        "\n",
        "    def reset(self):\n",
        "        '''\n",
        "        When running dqn:\n",
        "            observation: np.array\n",
        "                stack 4 last frames, shape: (84, 84, 4)\n",
        "        When running pg:\n",
        "            observation: np.array\n",
        "                current RGB screen of game, shape: (210, 160, 3)\n",
        "        '''\n",
        "        observation = self.env.reset()\n",
        "\n",
        "        return np.array(observation)\n",
        "\n",
        "\n",
        "    def step(self,action):\n",
        "        '''\n",
        "        When running dqn:\n",
        "            observation: np.array\n",
        "                stack 4 last preprocessed frames, shape: (84, 84, 4)\n",
        "            reward: int\n",
        "                wrapper clips the reward to {-1, 0, 1} by its sign\n",
        "                we don't clip the reward when testing\n",
        "            done: bool\n",
        "                whether reach the end of the episode?\n",
        "        When running pg:\n",
        "            observation: np.array\n",
        "                current RGB screen of game, shape: (210, 160, 3)\n",
        "            reward: int\n",
        "                if opponent wins, reward = +1 else -1\n",
        "            done: bool\n",
        "                whether reach the end of the episode?\n",
        "        '''\n",
        "        if not self.env.action_space.contains(action):\n",
        "            raise ValueError('Ivalid action!!')\n",
        "\n",
        "      \n",
        "            self.env.render()\n",
        "\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "\n",
        "        return np.array(observation), reward, done, info\n",
        "    def render(self):\n",
        "        self.env.render()\n",
        "\n",
        "\n",
        "    def get_action_space(self):\n",
        "        return self.action_space\n",
        "\n",
        "\n",
        "    def get_observation_space(self):\n",
        "        return self.observation_space\n",
        "\n",
        "\n",
        "    def get_random_action(self):\n",
        "        return self.action_space.sample()\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deep-Q-Network-Breakout  sample_data  video\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDrHtWLjSxUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQFDn8op9L9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class Agent(object):\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "\n",
        "\n",
        "    def make_action(self, observation, test=True):\n",
        "        \"\"\"\n",
        "        Return predicted action of your agent\n",
        "        This function must exist in agent\n",
        "        Input:\n",
        "            When running dqn:\n",
        "                observation: np.array\n",
        "                    stack 4 last preprocessed frames, shape: (84, 84, 4)\n",
        "            When running pg:\n",
        "                observation: np.array\n",
        "                    current RGB screen of game, shape: (210, 160, 3)\n",
        "        Return:\n",
        "            action: int\n",
        "                the predicted action from trained model\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
        "\n",
        "\n",
        "    def init_game_setting(self):\n",
        "        \"\"\"\n",
        "        Testing function will call this function at the begining of new game\n",
        "        Put anything you want to initialize if necessary\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Subclasses should implement this!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSzsOe4cTYNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Input, Conv2D, Flatten, Dense, LeakyReLU, Multiply, Maximum, Add, merge\n",
        "from keras.optimizers import RMSprop,Adam\n",
        "import keras.backend as K\n",
        "from keras.layers import Lambda\n",
        "\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "\n",
        "\n",
        "np.random.seed(1)\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(1)\n",
        "\n",
        "# reference : https://github.com/tokb23/dqn/blob/master/dqn.py\n",
        "\n",
        "class agent_DQN(Agent):\n",
        "    def __init__(self, env):\n",
        "        super(agent_DQN,self).__init__(env)\n",
        "        config = tf.ConfigProto()\n",
        "\n",
        "        set_session(tf.Session(config=config))\n",
        "\n",
        "        # parameters\n",
        "        self.frame_width = 84\n",
        "        self.frame_height = 84\n",
        "        self.num_steps =1000\n",
        "        self.state_length = 4\n",
        "        self.gamma = 0.95\n",
        "        self.exploration_steps = 3000\n",
        "        self.initial_epsilon = 1\n",
        "        self.final_epsilon = 0.2\n",
        "\n",
        "        self.batch_size = 256\n",
        "\n",
        "        self.learning_rate = 0.01\n",
        "\n",
        "\n",
        "        self.no_op_steps = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.opt = Adam(lr=self.learning_rate)\n",
        "            \n",
        "\n",
        "     \n",
        "        # environment setting\n",
        "        self.env = env\n",
        "        self.num_actions = env.action_space.n\n",
        "        \n",
        "        self.epsilon = self.initial_epsilon\n",
        "        self.epsilon_step = (self.initial_epsilon - self.final_epsilon) / self.exploration_steps\n",
        "        self.t = 0\n",
        "\n",
        "        # Input that is not used when fowarding for Q-value \n",
        "        # or loss calculation on first output of model \n",
        "        self.dummy_input = np.zeros((1,self.num_actions))\n",
        "        self.dummy_batch = np.zeros((self.batch_size,self.num_actions))\n",
        "\n",
        "        # for summary & checkpoint\n",
        "        self.total_reward = 0.0\n",
        "        self.total_q_max = 0.0\n",
        "        self.total_loss = 0.0\n",
        "        self.duration = 0\n",
        "        self.replay_memory = 10000\n",
        "        self.episode = 0\n",
        "        self.last_30_reward = deque()\n",
        "\n",
        "\n",
        "\n",
        "        # Create replay memory\n",
        "        self.replay_memory = deque()\n",
        "\n",
        "        # Create q network\n",
        "        self.q_network = self.build_network()\n",
        "\n",
        "        # Create target network\n",
        "        self.target_network = self.build_network()\n",
        "\n",
        "\n",
        "\n",
        "        # Set target_network weight\n",
        "\n",
        "            \n",
        "\n",
        "    def init_game_setting(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        while self.t <= self.num_steps:\n",
        "            \n",
        "            terminal = False\n",
        "            observation = self.env.reset()\n",
        "            for _ in range(random.randint(1, self.no_op_steps)):\n",
        "                last_observation = observation\n",
        "                observation, _, _, _ = self.env.step(0)  # Do nothing\n",
        "            while not terminal:\n",
        "                self.env.render()\n",
        "                last_observation = observation\n",
        "                action = self.make_action(last_observation,test=False)\n",
        "                observation, reward, terminal, _ = self.env.step(action)\n",
        "                self.run(last_observation, action, reward, terminal, observation)\n",
        "        show_video()\n",
        "                    \n",
        "\n",
        "\n",
        "    def make_action(self, observation, test=True):\n",
        "        \"\"\"\n",
        "        ***Add random action to avoid the testing model stucks under certain situation***\n",
        "        Input:\n",
        "            observation: np.array\n",
        "                stack 4 last preprocessed frames, shape: (84, 84, 4)\n",
        "\n",
        "        Return:\n",
        "            action: int\n",
        "                the predicted action from trained model\n",
        "        \"\"\"\n",
        "        if not test:\n",
        "            if self.epsilon >= random.random():\n",
        "               action = random.randrange(self.num_actions)\n",
        "            else:\n",
        "                action = np.argmax(self.q_network.predict([np.expand_dims(observation,axis=0),self.dummy_input])[0])\n",
        "            # Anneal epsilon linearly over time\n",
        "            if self.epsilon > self.final_epsilon:\n",
        "                self.epsilon -= self.epsilon_step\n",
        "\n",
        "\n",
        "        return action\n",
        "\n",
        "    def build_network(self):\n",
        "        # Consturct model\n",
        "        input_frame = Input(shape=(self.frame_width, self.frame_height, self.state_length))\n",
        "        action_one_hot = Input(shape=(self.num_actions,))\n",
        "        conv1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu')(input_frame)\n",
        "        conv2 = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')(conv1)\n",
        "        conv3 = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')(conv2)\n",
        "        flat_feature = Flatten()(conv3)\n",
        "        hidden_feature = Dense(512)(flat_feature)\n",
        "        lrelu_feature = LeakyReLU()(hidden_feature)\n",
        "        q_value_prediction = Dense(self.num_actions)(lrelu_feature)\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "        select_q_value_of_action = Multiply()([q_value_prediction,action_one_hot])\n",
        "        target_q_value = Lambda(lambda x:K.sum(x, axis=-1, keepdims=True),output_shape=lambda_out_shape)(select_q_value_of_action)\n",
        "        \n",
        "        model = Model(inputs=[input_frame,action_one_hot], outputs=[q_value_prediction, target_q_value])\n",
        "        \n",
        "        \n",
        "        # MSE loss on target_q_value only\n",
        "        model.compile(loss=['mse','mse'], loss_weights=[0.0,1.0],optimizer=self.opt)\n",
        "\n",
        "        return model        \n",
        "\n",
        "    def run(self, state, action, reward, terminal, observation):\n",
        "        next_state = observation\n",
        "\n",
        "        # Store transition in replay memory\n",
        "        self.replay_memory.append((state, action, reward, next_state, terminal))\n",
        "\n",
        "\n",
        "        if self.t >= 512:\n",
        "            # Train network\n",
        "\n",
        "            self.train_network()\n",
        "\n",
        "            # Update target network\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.total_reward += reward\n",
        "        self.total_q_max += np.max(self.q_network.predict([np.expand_dims(state,axis=0),self.dummy_input])[0])\n",
        "        self.duration += 1\n",
        "\n",
        "        if terminal:\n",
        "            # Observe the mean of rewards on last 30 episode\n",
        "            self.last_30_reward.append(self.total_reward)\n",
        "            if len(self.last_30_reward)>30:\n",
        "                self.last_30_reward.popleft()\n",
        "\n",
        "\n",
        "            print('EPISODE: {0:6d} / TIMESTEP: {1:8d} / DURATION: {2:5d} / EPSILON: {3:.5f} / AVG_REWARD: {4:2.3f} / AVG_MAX_Q: {5:2.4f} / AVG_LOSS: {6:.5f}  '.format(\n",
        "                self.episode + 1, self.t, self.duration, self.epsilon,\n",
        "                np.mean(self.last_30_reward), self.total_q_max / float(self.duration),\n",
        "                self.total_loss / (float(self.duration) )))\n",
        "            print('EPISODE: {0:6d} / TIMESTEP: {1:8d} / DURATION: {2:5d} / EPSILON: {3:.5f} / AVG_REWARD: {4:2.3f} / AVG_MAX_Q: {5:2.4f} / AVG_LOSS: {6:.5f} '.format(\n",
        "                self.episode + 1, self.t, self.duration, self.epsilon,\n",
        "                np.mean(self.last_30_reward), self.total_q_max / float(self.duration),\n",
        "                self.total_loss / (float(self.duration)  )))\n",
        "\n",
        "            # Init for new game\n",
        "            self.total_reward = 0\n",
        "            self.total_q_max = 0\n",
        "            self.total_loss = 0\n",
        "            self.duration = 0\n",
        "            self.episode += 1\n",
        "\n",
        "        self.t += 1\n",
        "\n",
        "    def train_network(self):\n",
        "        state_batch = []\n",
        "        action_batch = []\n",
        "        reward_batch = []\n",
        "        next_state_batch = []\n",
        "        terminal_batch = []\n",
        "        y_batch = []\n",
        "\n",
        "\n",
        "        # Sample random minibatch of transition from replay memory\n",
        "        minibatch = random.sample(self.replay_memory, self.batch_size)\n",
        "        for data in minibatch:\n",
        "            state_batch.append(data[0])\n",
        "            action_batch.append(data[1])\n",
        "            reward_batch.append(data[2])\n",
        "            next_state_batch.append(data[3])\n",
        "            terminal_batch.append(data[4])\n",
        "\n",
        "        # Convert True to 1, False to 0\n",
        "        terminal_batch = np.array(terminal_batch) + 0\n",
        "        # Q value from target network\n",
        "        target_q_values_batch = self.q_network.predict([list2np(next_state_batch),self.dummy_batch])[0]\n",
        "\n",
        "        # create Y batch depends on dqn or ddqn\n",
        "\n",
        "        \n",
        "        y_batch = reward_batch + (1 - terminal_batch) * self.gamma * np.max(target_q_values_batch, axis=-1)\n",
        "        \n",
        "        a_one_hot = np.zeros((self.batch_size,self.num_actions))\n",
        "        for idx,ac in enumerate(action_batch):\n",
        "            a_one_hot[idx,ac] = 1.0\n",
        "\n",
        "        loss = self.q_network.train_on_batch([list2np(state_batch),a_one_hot],[self.dummy_batch,y_batch])\n",
        "\n",
        "        self.total_loss += loss[1]\n",
        "\n",
        "def list2np(in_list):\n",
        "    return np.float32(np.array(in_list))\n",
        "\n",
        "def lambda_out_shape(input_shape):\n",
        "    shape = list(input_shape)\n",
        "    shape[-1] = 1\n",
        "    return tuple(shape)\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt6ZjfY7Fbja",
        "colab_type": "text"
      },
      "source": [
        "ای قسمت برای اجرای کد است.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMh7ObxKFcft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Environment(atari_wrapper=True)\n",
        "\n",
        "print(env.observation_space)\n",
        "agent = agent_DQN(env)\n",
        "\n",
        "\n",
        "agent.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQvCxpp1MJug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}